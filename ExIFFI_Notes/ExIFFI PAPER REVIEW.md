---
Created: 31 January 2024
Class: Anomaly Detection
Type: Notes
---
Annotate here the main comments contained in the review of the EXIFFI paper and write down the things to do to organize the re writing process of the paper. 

> [!warning] 
> Deadline for submitting the reviewed paper -> 31 March 2024  
# Editor Comments

1. Address concerns regarding the superiority of ExIFFI over distilling feature importance from traditional isolation-based anomaly detection methods like iForest and EIF. Provide a more detailed explanation of why the proposed method is advantageous.

> [!todo] 
> Expand the explanation and intuition under the EIF+ model and why it is better than IF and EIF

> [!done] 
> Using the contamination plots we can show that EIF+ is better than the other models. 
> 

> [!todo] 
> - Add the performance metrics for the final table (e.g. Precision, Recall, Average Precision,....)
> - Justify the superiority of  `EIF+` commenting the results in the paper. 

2.  Expand the discussion of related work on interpretation methods for unsupervised models. Include recent studies. Additionally, conduct more ablation studies to analyze the impact of design choices in EIF+ and ExIFFI. Perform performance comparisons with state-of-the-art baselines published more recently to strengthen justification.

> [!todo] 
> - Expand the related work section talking about interpretation methods used in unsupervised models (i.e. not only interpretation method in Anomaly Detection but also on other unsupervised learning tasks) → [[ExIFFI PAPER REVIEW#Papers for Related Work|papers to cite]]. 
> - Ablation study on EIF+ and ExIFFI → Try to see what happens removing some of the design choices we made in the model 
> - Comparison with state of the art baselines recently published -> We already have the comparison with DIFFI. Search other models to compare ExIFFI to → **Deep Isolation Forest (DIF),** **ECOD**. 

> [!done] 
> - Comparison of `EIF+` to `IF,EIF,DIF,AutoEncoder` in the contamination plots
> - Add this comparison also on the final performance table 
> - `EIF+` Ablation Study:
> 	- Contamination plots done 

> [!todo] 
> - Include references to other interpretation methods in the Related Work section 
> - `EIF+` Ablation Study 
> 	- Try to see what happens with different values of $\eta$ ? For example do a plot `Average Precision vs eta` 

3. Acknowledge concerns about the assumption that labeled anomalies truly deviate from the distribution in real datasets. Consider discussing the time complexity of EIF+ and its scalability to larger datasets. Introduce user studies to assess whether ExIFFI explanations align well with human interpretations, addressing concerns about evaluations limited to datasets without ground truth anomalies. 

> [!todo] 
> - Time analysis to see how ExIFFI scales (in terms of execution time) on larger datasets -> find these larger datasets. Here we can use the Parallelized version of ExIFFI I am working on with Francesco
> - Introduce User Studies to see how ExIFFI aligns with human explanations -> so we need to find some datasets where there are ground truth anomalies 

> [!done] 
> - Time Analysis → We computed the `fit`, `predict` and `importances` time on all the different datasets and we will report all the times in a final table. We do not have industrial datasets but some of the ones we used are pretty large → so probably we need to do a run on the complete version of `moodify` or find an industrial dataset. 
> - Explain why User Study cannot be done and that we do Feature Selection Proxy Task instead 

- Time comparison with Tree SHAP. 

> [!info] Definition of User Studies from Chat GPT
>  When we talk about user studies to assess whether an interpretation algorithm aligns well with human explanations, we are referring to a process of evaluating how well the output of the algorithm can be understood and accepted by human users. Interpretation algorithms are often used in machine learning models to provide insights into why a particular decision or prediction was made. These algorithms generate explanations or visualizations that aim to make the model's decision-making process more transparent and interpretable to users.
>  User studies in this context involve gathering feedback from human users to determine if the explanations generated by the algorithm are meaningful, accurate, and align with human understanding.

^e08017

For an example of a User Study see [here](http://199.247.16.97/) 
# Reviewer 1 Comments 

Recommendation: Major Revision

I have the following concerns,  
1. As iForest and EIF are not deep learning-based methods, it is possible to distill feature importance from iTrees by considering which features are used for partition. Could the authors please further explain the superiority of the proposed explanation method?  

> [!todo] 
> Same as point 1 of [[ExIFFI PAPER REVIEW#Editor Comments|Editor Comments]]. 
> Explain well that IF,EIF are not intrinsically explainable because the feature are chosen at random at every cut. Cite DIFFI in this explanation. 

2. In the introduction section, the authors are encouraged to further explain the basic insights of the proposed method. Only contributions are listed (contributions are not very specific as well, it is better to further explain why the proposed EIF+ is better), and the readers may also need to know how the proposed method achieves these merits. Please refer to some papers published on top-tier venues; the writing logic of the introduction part could be improved.  

> [!todo] 
> Essentially he/she is saying that the Introduction can be written better. It is suggesting again to explain further why EIF+ is better.  
> - Check weather the papers in [[ExIFFI PAPER REVIEW#Papers for Related Work|Papers to cite]] are in top-tier venues (check how much citations they have). 
> - Obviously include DIF in the cited papers. 

4. Some recent studies on isolation-based anomaly detection like [1] should be discussed → **Deep Isolation Forest (DIF)**
# Reviewer 2 Comments

Recommendation: Major Revision
## Paper Strengths 

1. The paper tackles the important problem of providing explanations for unsupervised anomaly detection models. Interpretability is critical for gaining user trust and facilitating applications.  
2. A new model-specific interpretation algorithm called ExIFFI is proposed to interpret predictions made by EIF. Feature importance scores provide useful insights into anomaly patterns.  
3. Detailed analysis and visualizations like score maps aid understanding of ExIFFI results and how features influence anomalies. 
4. Code is openly available to facilitate reproducibility of the results.
## Paper Weaknesses

1. Limited discussion of related work on interpretation methods for unsupervised models.  

> [!todo] 
> Same as point 2 of [[ExIFFI PAPER REVIEW#Editor Comments|Editor Comments]] 

2. More ablation studies are needed to analyze the impact of different design choices in EIF+ and ExIFFI, such as variations to the normalization distributions and hyperparameter sensitivity analysis. 

> [!todo] 
> Like point 2 of [[ExIFFI PAPER REVIEW#Editor Comments|Editor Comments]]. Here he/she also suggests on what to do the ablation studies:
> - Variations to the normalization distributions:
> 	- Use different distributions for the Normalization part (other than `StandardScaler`) ?
> 	- Or use different distributions to generate the synthetic datasets?
> 	- Or use different distributions to draw the normal vectors used to generate the hyperplanes?
> - Hyperparameter sensitivity analysis: 
> 	- See how the model performance change with variations on the hyperparameter (i.e. number of trees, number of runs of `compute_imps`)

3. Performance comparisons with state-of-the-art baselines published more recently are missing. It would help justify the usefulness of EIF+/ExIFFI if compared against more recent methods.  

> [!todo] 
> Same as point 2 of [[ExIFFI PAPER REVIEW#Editor Comments|Editor Comments]] 

4.  For real datasets, labeled anomalies may not truly deviate from the distribution as assumed by isolation forest models. This undermines the evaluation of interpretation results when training on entire data. 

> [!todo] 
> This is the problem we have found in the real world dataset used and that forced us to use the *Scenario II* training strategy. Here we should find datasets with ground truth anomalies to test the model and not Multi Class Classification datasets adapted to Anomaly Detection -> that is the hard part.  

5. Lack of discussions on the time complexity of EIF+ and whether it can scale to industrial-sized datasets. 

> [!todo] 
> Same as point 3 of [[ExIFFI PAPER REVIEW#Editor Comments|Editor Comments]]. Use Parallel ExIFFI and find larger datasets.
> - Do experiments on `moodify` with different sized of the `downsample` function (7500,10000,15000,30000,50000,100000,200000,300000) 

6. Evaluations are limited to real-world datasets without ground truth anomalies. It is unclear whether ExIFFI explanations align well with human interpretations. User studies would provide a better evaluation of interpretability.

> [!todo] 
> Same as the second part of point 3 of [[ExIFFI PAPER REVIEW#Editor Comments|Editor Comments]]
# Papers

## Papers for Related Work

- ***A Survey on Explainable Anomaly Detection*** → Here we can find some other interpretation algorithms in the field of anomaly detection to cite in the Related Work section. In this survey there is also DIFFI. 
- ***Why are you weird? Infusing Interpretability in Isolation Forest for Anomaly Detection*** → This is another interpretability method for the Isolation Forest model. It is surely a paper to cite and also look at the interpretation algorithm it uses in the Related Work section. It may also be a model to compare ExIFFI/DIFFI to. 
- ***Interpretable Anomaly Detection for Knowledge Discovery in Semiconductor Manufacturing*** → This is a paper by Susto and Mattia Carletti (the inventor of DIFFI). This is good for the Related Work section because it is an application of DIFFI in the industrial world. 
- ***A New Interpretable Unsupervised Anomaly Detection Method Based on Residual Explanation*** -> Interpretation method on Autoencoders for Anomaly Detection. Explanations are obtained from a deviation analysis of reconstructed input features. 
- ***DeepAID: Interpreting and Improving Deep Learning-based Anomaly Detection in Security Applications*** → from the abstract they talk about an interpretation method for Unsupervised DNN that will then be used in DL-based Anomaly Detection systems. 
- ***Multivariate Time Series Anomaly Detection and Interpretation using Hierarchical Inter-Metric and Temporal Embedding***→ This paper is also reported in *A Survey on Explainable Anomaly Detection*. Here we are talking about Anomaly Detection for Multi variate Time Series. This approach is based on hierarchical Variational Autoencoders (HVAE) and Markov Chain Monte Carlo (MCMC). Given an anomaly, they set up a MCMC-based method to find a set of the most anomalous metrics as explanations.
- ***DAEMON: Unsupervised Anomaly Detection and Interpretation for Multivariate Time Series*** → This method still works on Multi Variate Time Series and it trains an Adversarial AutoEncoder (AAE) to learn the typical pattern of multivariate time series, and then use the reconstruction error to identify and explain anomalies.
- ***Interpreting Unsupervised Anomaly Detection in Security via Rule Extraction*** → Here we are in the security setting. They proposed the concept of *distribution decomposition rules* that decompose the complex distribution of normal data into multiple compositional distributions. To find these rules they defined an unsupervised Interior Clustering Tree that incorporates the model prediction into the splitting criteria. Then, we propose the Compositional Boundary Exploration (CBE) algorithm to obtain the boundary inference rules that estimate the decision boundary of the original model on each compositional distribution. 

% The need for explainability is particularly required in deep learning based Anomaly Detection methods which are considered, because of their inherent structure, black-box models. The majority of deep learning based AD models are based on the Anomaly Autoencoder (AE) model \cite{AutoEncoder}. Oliveira et al. introduced RXP (Residual eXPlainer) \cite{RXP}, an interpretable method to deal with the shortcomings of AE-based AD models where the explanations are produced performing a deviation analysis of the reconstructed input features. 

% ECOD (Empirical-Cumulative-distribution-based Outlier Detection) \cite{ECOD} is another common approach for Anomaly Detection based on the empirical cumulative distribution of the data points and the fact that outliers are points placed in the tails of the estimated distribution. Differently from other XAD approaches ECOD has the advantage of possessing an intrinsic local interpretability. In fact, given a data point, it is necessary to simply observe the estimated tail probabilities of each dimension to have a clear measure of the contribution of each one of them in the detection of abnormal samples.
## More recent papers

- ***A self-supervised anomaly detection algorithm with interpretability***
- ***Enhancing anomaly detection accuracy and interpretability in low-quality and class imbalanced data: A comprehensive approach***
- **An interpretable autoencoder for semi-supervised anomaly detection***

> [!note] 
> There are a lot of interpretation algorithm on AE-based (AutoEncoder based) Anomaly Detection. In fact AE is the most used DL method for Anomaly Detection. Differently from Isolation Forest based methods (like ExIFFI) it requires much more data to be trained so we can use this to say that we are proposing an interpretation method for a kind of model that does not have it (expect for DIFFI). 
> Moreover we can use these citations to motivate the novelty of our approach that is based on the peculiar structure of the Isolation Forest model. 
## Important Papers 

- ***Deep Isolation Forest for Anomaly Detection*** (DIF)→ This is the paper suggested by the reviewers. We can use this model to compare its performances with the ExIFFI performances. This model may have better performances but, because it uses some NN to transform the inputs and put them into another representation, we this is probaax=sns.barplot(x='num_samples',y='real_time',data=df_100,width=0.5)
- ***ECOD*** → Intrinsic interpretable method. Other possible model to compare. 

> [!tip] 
> After having created an ensemble of representations using the randomly initialized DNN in the paper they adopt the simple axis-parallel cuts used in IF. But what if we try to extend this model using the cuts used in EIF or EIF+ ? May it work better or the newly created representations are already optimally changing the points distribution so that the simple IF cuts are already enough? 
>   

- ***PIDForest: Anomaly Detection via Partial Identification*** (PID) → This one can ba another model to compare to EIF+. 
- INNE Paper (see PyOD)
- [Autoencoder for AD]([https://www.nature.com/articles/s42256-023-00620-w.pdf?pdf=button%20sticky](https://www.nature.com/articles/s42256-023-00620-w.pdf?pdf=button%20sticky))
- ***Evaluation of post-hoc interpretability methods in time-series classification*** → This paper introduces some new metrics to evaluate interpretability models. The paper works on Time Series data and the interpretability methods evaluate the relevance of different time stamps -> we are working with tabular data and for use the time stamps will be the features. 
- ***On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study*** → This dataset contains a section that describes some datasets with semantic outliers -> this is what we need, some outliers with a semantic meaning so that we can see if the interpretation done by ExIFFI aligns with the domain knowledge. 
-  ***The precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets*** by T. Saito, M. Rehmsmeier: Cite this paper to support our decision of using the Average Precision as the metric to compare the performances of different models in the Ablation studies, Feature Selection and Contamination Plots. 
- ***The inadequacy of Shapley Values for Explainability*** → We can use this paper to justify the reason why we did not use SHAP as an interpretability method to compare to `EXIFFI,EXIFFI+`. 

## Benchmark Datasets 

In the DIF paper very big datasets are used. This can also be helpful for the comparison of ExIFFI with DIF because we can just take the results reported in the DIF paper instead of re running the experiments (so we just have to run the ExIFFI experiments):

- *Analysis,Backdoor,DoS* and *Exploits* -> taken from an intrusion detection benchmark UNSW_NB 15 -> their shape is more or less `(95.000,197)`. This datasets may be useful to see how ExIFFI behaves this very high number of features. 
	- *Backdoor* is on ADBench Git repo → ok
- *R8* → highly unbalanced text classification dataset → shape `(3974,9468)` 
- *Cover* → Ecology domain → ok
- *Fraud* → Fraudulent Credit Card transactions → shape `(284.807,30)` → ok  
- *Pageblock* → shape `(5393,11)`
- *Thrombin* → ultrahigh-dimensional anomaly detection→ shape `(1909,139352)`

Datasets from Table 2 of *On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study*: 

- *Arrhythmia* → shape `(450,259)` → ok 
- *HeartDisease* → shape `(270,120)`
- *Hepatitis*  → shape `(80,13)` → ok 
- *InternetAds* → shape `(3264,1555)` → ok 
- *PageBlocks* → shape `(5473,10)` → ok,
- *Parkinson* → shape `(195,22)`
- *SpamBase* → shape `(4601,57)` → ok 
- *Stamps* → shape `(340,9)`
- *Wilt* → shape `(4839,5)` → ok

# Important Considerations

> [!important] Efficiency of EIF+
>  One thing we can say to support our new EIF+ approach is that it is also faster in training with the respect to EIF because doing the cuts using the distribution $N(mean(X),\eta \ std(X))$ it is more likely that we divide the cluster of points in two halves with the same/similar number of points on the two sides. On the other hand in EIF we use $U(min(X),max(X))$ so there are the same chances of dividing in two halves of the similar size but also the chance of dividing into two halves of very different sizes → in this case than the number of cuts needed to isolate all the points (or getting to the `max_depth`) is much higher. 
>  We have noticed this thing seeing that the `fit` method in EIF takes much more time than the one in EIF+. 
>  We have also computed the average number of nodes created in all the trees created during the `fit` of these two methods:
>  - EIF, wine, 100 trees:
> 	 - avg number of nodes: 13610
> 	 - std number of nodes: 590
> - DIFFI, wine, 100 trees  
> 	- avg number of nodes: 517
> 	- std number of nodes: 31

> [!important] Execution time of `fit` on different datasets
>  The execution time of the `fit` method in EIF strangely takes more time on `wine` (that is the smallest dataset we have) than in `moodify` (that is the biggest dataset we have). This is due to the fact that the trees that are built in the `fit` method for `wine` are much more complex than the ones built for `moodify` and this depends on the structure of the dataset. It means that is easier to isolate the data in `moodify` than in `wine`. 
>  However if we try the `fit` method of EIF+ it takes more or less the same amount of time because thanks to the intelligent cuts performed by EIF+ the time difference in the training due to the different distribution of the data is reduced. 
>  
>  - EDIFFI fitting times (100 trees):  
> 	 - `wine` = 0.7s
> 	 - `moodify` = 1.68s
> 	 - `difference` = 2.4x
> - EIF fitting times (100 trees):
> 	- `wine` = 62s
> 	- `moodify` = 5.9s
> 	- `difference` = 10.5x

> [!todo] `normal_median` Distribution
>  Sampling from the distribution $N(median(X),\eta \ std(X))$ maybe we can better even better cuts because the median is not influenced by the outliers and will provide cuts that will divide in exact halves the dataset with higher probabilities. 

# `ExIFFI` Final Review Notes

Le cose da sistemare secondo la review reject sono:

- Punto 1 -> Ridurre di molto la lunghezza facendo una cosa molto piu concisa in modo che sia piu facile da presentare e da capire il metodo per I lettori. 
- Punto 2 -> Qua non capisco bene cosa intenda con manually designed features. Poi dice che 
- Punto 3-> Possiamo supportare l'applicabilità di `EXIFFI` a non solo `IF` e dati tabulari visto I risultati che stiamo ottenendo da `TEP` e `PIADE`. Qua bisogna capire se si possono includere I risultati su questi due nella nuova submission di `EXIFFI` o fare cmq il paper industriale a parte e citarlo. 
- Punto 4 -> Nella parte sperimentale bisogna includere il paragone con qualche modello di interpretability piu recente. Potremmo cmq mettere `ACME` e `DIFFI` (anche se è sempre Gian vs Gian vs Gian). Forse le varie versioni di `SHAP` sono un po troppo vecchie (Hanno detto degli ultimi due anni) -> provare a cercare qualche modello di interpretability per AD ma non credo ce ne siano molti. Si potrebbe considerare `ECOD` (intrinsically interpretable ma solo Local).


