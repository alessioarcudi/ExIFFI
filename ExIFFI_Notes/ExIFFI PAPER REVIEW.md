---
Created: 31 January 2024
Class: Anomaly Detection
Type: Notes
---
Annotate here the main comments contained in the review of the [[DIFFI and EXIFFI - Interpretability on the Isolation Forest model|ExIFFI]] paper and write down the things to do to organize the re writing process of the paper. 

> [!warning] 
> Deadline for submitting the reviewed paper -> 31 March 2024  
# Editor Comments

1. Address concerns regarding the superiority of ExIFFI over distilling feature importance from traditional isolation-based anomaly detection methods like iForest and EIF. Provide a more detailed explanation of why the proposed method is advantageous.

> [!todo] 
> Expand the explanation and intuition under the EIF+ model and why it is better than IF and EIF. 

2.  Expand the discussion of related work on interpretation methods for unsupervised models. Include recent studies. Additionally, conduct more ablation studies to analyze the impact of design choices in EIF+ and ExIFFI. Perform performance comparisons with state-of-the-art baselines published more recently to strengthen justification.

> [!todo] 
> - Expand the related work section talking about interpretation methods used in unsupervised models (i.e. not only interpretation method in Anomaly Detection but also on other unsupervised learning tasks) → [[ExIFFI PAPER REVIEW#Papers for Related Work|papers to cite]]. 
> - Ablation study on EIF+ and ExIFFI → Try to see what happens removing some of the design choices we made in the model 
> - Comparison with state of the art baselines recently published -> We already have the comparison with DIFFI. Search other models to compare ExIFFI to → **Deep Isolation Forest (DIF).** 

3. Acknowledge concerns about the assumption that labeled anomalies truly deviate from the distribution in real datasets. Consider discussing the time complexity of EIF+ and its scalability to larger datasets. Introduce user studies to assess whether ExIFFI explanations align well with human interpretations, addressing concerns about evaluations limited to datasets without ground truth anomalies. 

> [!todo] 
> - Time analysis to see how ExIFFI scales (in terms of execution time) on larger datasets -> find these larger datasets. Here we can use the Parallelized version of ExIFFI I am working on with Francesco
> - Introduce User Studies to see how ExIFFI aligns with human explanations -> so we need to find some datasets where there are ground truth anomalies 

> [!info] Definition of User Studies from Chat GPT
>  When we talk about user studies to assess whether an interpretation algorithm aligns well with human explanations, we are referring to a process of evaluating how well the output of the algorithm can be understood and accepted by human users. Interpretation algorithms are often used in machine learning models to provide insights into why a particular decision or prediction was made. These algorithms generate explanations or visualizations that aim to make the model's decision-making process more transparent and interpretable to users.
>  User studies in this context involve gathering feedback from human users to determine if the explanations generated by the algorithm are meaningful, accurate, and align with human understanding.

^e08017

For an example of a User Study see [here](http://199.247.16.97/) 
# Reviewer 1 Comments 

Recommendation: Major Revision

I have the following concerns,  
1. As iForest and EIF are not deep learning-based methods, it is possible to distill feature importance from iTrees by considering which features are used for partition. Could the authors please further explain the superiority of the proposed explanation method?  

> [!todo] 
> Same as point 1 of [[ExIFFI PAPER REVIEW#Editor Comments|Editor Comments]] 

2. In the introduction section, the authors are encouraged to further explain the basic insights of the proposed method. Only contributions are listed (contributions are not very specific as well, it is better to further explain why the proposed EIF+ is better), and the readers may also need to know how the proposed method achieves these merits. Please refer to some papers published on top-tier venues; the writing logic of the introduction part could be improved.  papers to cite

> [!todo] 
> Essentially he/she is saying that the Introduction can be written better. It is suggesting again to explain further why EIF+ is better.  

4. Some recent studies on isolation-based anomaly detection like [1] should be discussed → **Deep Isolation Forest (DIF)**
# Reviewer 2 Comments

Recommendation: Major Revision
## Paper Strengths 

1. The paper tackles the important problem of providing explanations for unsupervised anomaly detection models. Interpretability is critical for gaining user trust and facilitating applications.  
2. A new model-specific interpretation algorithm called ExIFFI is proposed to interpret predictions made by EIF. Feature importance scores provide useful insights into anomaly patterns.  
3. Detailed analysis and visualizations like score maps aid understanding of ExIFFI results and how features influence anomalies. 
4. Code is openly available to facilitate reproducibility of the results.
## Paper Weaknesses

1. Limited discussion of related work on interpretation methods for unsupervised models.  

> [!todo] 
> Same as point 2 of [[ExIFFI PAPER REVIEW#Editor Comments|Editor Comments]] 

2. More ablation studies are needed to analyze the impact of different design choices in EIF+ and ExIFFI, such as variations to the normalization distributions and hyperparameter sensitivity analysis. 

> [!todo] 
> Like point 2 of [[ExIFFI PAPER REVIEW#Editor Comments|Editor Comments]]. Here he/she also suggests on what to do the ablation studies:
> - Variations to the normalization distributions:
> 	- Use different distributions for the Normalization part (other than `StandardScaler`) ?
> 	- Or use different distributions to generate the synthetic datasets?
> 	- Or use different distributions to draw the normal vectors used to generate the hyperplanes?
> - Hyperparameter sensitivity analysis: 
> 	- See how the model performance change with variations on the hyperparameter (i.e. number of trees, number of runs of `compute_imps`)

3. Performance comparisons with state-of-the-art baselines published more recently are missing. It would help justify the usefulness of EIF+/ExIFFI if compared against more recent methods.  

> [!todo] 
> Same as point 2 of [[ExIFFI PAPER REVIEW#Editor Comments|Editor Comments]] 

4.  For real datasets, labeled anomalies may not truly deviate from the distribution as assumed by isolation forest models. This undermines the evaluation of interpretation results when training on entire data. 

> [!todo] 
> This is the problem we have found in the real world dataset used and that forced us to use the *Scenario II* training strategy. Here we should find datasets with ground truth anomalies to test the model and not Multi Class Classification datasets adapted to Anomaly Detection -> that is the hard part.  

5. Lack of discussions on the time complexity of EIF+ and whether it can scale to industrial-sized datasets. 

> [!todo] 
> Same as point 3 of [[ExIFFI PAPER REVIEW#Editor Comments|Editor Comments]]. Use Parallel ExIFFI and find larger datasets. 

6. Evaluations are limited to real-world datasets without ground truth anomalies. It is unclear whether ExIFFI explanations align well with human interpretations. User studies would provide a better evaluation of interpretability.

> [!todo] 
> Same as the second part of point 3 of [[ExIFFI PAPER REVIEW#Editor Comments|Editor Comments]]
# Papers

## Papers for Related Work

- ***A Survey on Explainable Anomaly Detection*** → Here we can find some other interpretation algorithms in the field of anomaly detection to cite in the Related Work section. In this survey there is also DIFFI. 
- ***Why are you weird? Infusing Interpretability in Isolation Forest for Anomaly Detection*** → This is another interpretability method for the Isolation Forest model. It is surely a paper to cite and also look at the interpretation algorithm it uses in the Related Work section. It may also be a model to compare ExIFFI/DIFFI to. 
- ***Interpretable Anomaly Detection for Knowledge Discovery in Semiconductor Manufacturing*** → This is a paper by Susto and Mattia Carletti (the inventor of DIFFI). This is good for the Related Work section because it is an application of DIFFI in the industrial world. 
- ***A New Interpretable Unsupervised Anomaly Detection Method Based on Residual Explanation*** -> Interpretation method on Autoencoders for Anomaly Detection. Explanations are obtained from a deviation analysis of reconstructed input features. 
- ***DeepAID: Interpreting and Improving Deep Learning-based Anomaly Detection in Security Applications*** → from the abstract they talk about an interpretation method for Unsupervised DNN that will then be used in DL-based Anomaly Detection systems. 
- ***Multivariate Time Series Anomaly Detection and Interpretation using Hierarchical Inter-Metric and Temporal Embedding***→ This paper is also reported in *A Survey on Explainable Anomaly Detection*. Here we are talking about Anomaly Detection for Multi variate Time Series. This approach is based on hierarchical Variational Autoencoders (HVAE) and Markov Chain Monte Carlo (MCMC). Given an anomaly, they set up a MCMC-based method to find a set of the most anomalous metrics as explanations.
- ***DAEMON: Unsupervised Anomaly Detection and Interpretation for Multivariate Time Series*** → This method still works on Multi Variate Time Series and it trains an Adversarial AutoEncoder (AAE) to learn the typical pattern of multivariate time series, and then use the reconstruction error to identify and explain anomalies.
- ***Interpreting Unsupervised Anomaly Detection in Security via Rule Extraction*** → Here we are in the security setting. They proposed the concept of *distribution decomposition rules* that decompose the complex distribution of normal data into multiple compositional distributions. To find these rules they defined an unsupervised Interior Clustering Tree that incorporates the model prediction into the splitting criteria. Then, we propose the Compositional Boundary Exploration (CBE) algorithm to obtain the boundary inference rules that estimate the decision boundary of the original model on each compositional distribution. 
## More recent papers

- ***A self-supervised anomaly detection algorithm with interpretability***
- ***Enhancing anomaly detection accuracy and interpretability in low-quality and class imbalanced data: A comprehensive approach***
- **An interpretable autoencoder for semi-supervised anomaly detection***

> [!note] 
> There are a lot of interpretation algorithm on AE-based (AutoEncoder based) Anomaly Detection. In fact AE is the most used DL method for Anomaly Detection. Differently from Isolation Forest based methods (like ExIFFI) it requires much more data to be trained so we can use this to say that we are proposing an interpretation method for a kind of model that does not have it (expect for DIFFI). 
> Moreover we can use these citations to motivate the novelty of our approach that is based on the peculiar structure of the Isolation Forest model. 
## Important Papers 

- ***Deep Isolation Forest for Anomaly Detection*** (DIF)→ This is the paper suggested by the reviewers. We can use this model to compare its performances with the ExIFFI performances. This model may have better performances but, because it uses some NN to transform the inputs and put them into another representation, we this is probaax=sns.barplot(x='num_samples',y='real_time',data=df_100,width=0.5)

> [!tip] 
> After having created an ensemble of representations using the randomly initialized DNN in the paper they adopt the simple axis-parallel cuts used in IF. But what if we try to extend this model using the cuts used in EIF or EIF+ ? May it work better or the newly created representations are already optimally changing the points distribution so that the simple IF cuts are already enough? 
>   

- ***PIDForest: Anomaly Detection via Partial Identification*** (PID) → This one can ba another model to compare to EIF+. 
- INNE Paper (see PyOD)
- [Autoencoder for AD]([https://www.nature.com/articles/s42256-023-00620-w.pdf?pdf=button%20sticky](https://www.nature.com/articles/s42256-023-00620-w.pdf?pdf=button%20sticky))
- ***Evaluation of post-hoc interpretability methods in time-series classification*** → This paper introduces some new metrics to evaluate interpretability models. The paper works on Time Series data and the interpretability methods evaluate the relevance of different time stamps -> we are working with tabular data and for use the time stamps will be the features. 
- ***On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study*** → This dataset contains a section that describes some datasets with semantic outliers -> this is what we need, some outliers with a semantic meaning so that we can see if the interpretation done by ExIFFI aligns with the domain knowledge. 

## Benchmark Datasets 

In the DIF paper very big datasets are used. This can also be helpful for the comparison of ExIFFI with DIF because we can just take the results reported in the DIF paper instead of re running the experiments (so we just have to run the ExIFFI experiments):

- *Analysis,Backdoor,DoS* and *Exploits* -> taken from an intrusion detection benchmark UNSW_NB 15 -> their shape is more or less `(95.000,197)`. This datasets may be useful to see how ExIFFI behaves this very high number of features. 
	- *Backdoor* is on ADBench Git repo → ok
- *R8* → highly unbalanced text classification dataset → shape `(3974,9468)` 
- *Cover* → Ecology domain → ok
- *Fraud* → Fraudulent Credit Card transactions → shape `(284.807,30)` → ok  
- *Pageblock* → shape `(5393,11)`
- *Thrombin* → ultrahigh-dimensional anomaly detection→ shape `(1909,139352)`

Datasets from Table 2 of *On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study*: 

- *Arrhythmia* → shape `(450,259)` → ok 
- *HeartDisease* → shape `(270,120)`
- *Hepatitis*  → shape `(80,13)` → ok 
- *InternetAds* → shape `(3264,1555)` → ok 
- *PageBlocks* → shape `(5473,10)` → ok,
- *Parkinson* → shape `(195,22)`
- *SpamBase* → shape `(4601,57)` → ok 
- *Stamps* → shape `(340,9)`
- *Wilt* → shape `(4839,5)` → ok

# Important Considerations

> [!important] Efficiency of EIF+
>  One thing we can say to support our new EIF+ approach is that it is also faster in training with the respect to EIF because doing the cuts using the distribution $N(mean(X),\eta \ std(X))$ it is more likely that we divide the cluster of points in two halves with the same/similar number of points on the two sides. On the other hand in EIF we use $U(min(X),max(X))$ so there are the same chances of dividing in two halves of the similar size but also the chance of dividing into two halves of very different sizes → in this case than the number of cuts needed to isolate all the points (or getting to the `max_depth`) is much higher. 
>  We have noticed this thing seeing that the `fit` method in EIF takes much more time than the one in EIF+. 
>  We have also computed the average number of nodes created in all the trees created during the `fit` of these two methods:
>  - EIF, wine, 100 trees:
> 	 - avg number of nodes: 13610
> 	 - std number of nodes: 590
> - DIFFI, wine, 100 trees  
> 	- avg number of nodes: 517
> 	- std number of nodes: 31

> [!important] Execution time of `fit` on different datasets
>  The execution time of the `fit` method in EIF strangely takes more time on `wine` (that is the smallest dataset we have) than in `moodify` (that is the biggest dataset we have). This is due to the fact that the trees that are built in the `fit` method for `wine` are much more complex than the ones built for `moodify` and this depends on the structure of the dataset. It means that is easier to isolate the data in `moodify` than in `wine`. 
>  However if we try the `fit` method of EIF+ it takes more or less the same amount of time because thanks to the intelligent cuts performed by EIF+ the time difference in the training due to the different distribution of the data is reduced. 
>  
>  - EDIFFI fitting times (100 trees):  
> 	 - `wine` = 0.7s
> 	 - `moodify` = 1.68s
> 	 - `difference` = 2.4x
> - EIF fitting times (100 trees):
> 	- `wine` = 62s
> 	- `moodify` = 5.9s
> 	- `difference` = 10.5x

> [!todo] `normal_median` Distribution
>  Sampling from the distribution $N(median(X),\eta \ std(X))$ maybe we can better even better cuts because the median is not influenced by the outliers and will provide cuts that will divide in exact halves the dataset with higher probabilities. 
# Things TO DO

> [!note] Color Code
> - <span style="color:red;"> Red  → Davide</span>
> - <span style="color:green;"> Green  → Alessio</span>
> - <span style="color:yellow;"> Yellow  → Davide and Alessio</span>
>  

- [x] [Possible solution to the Zoom Share Screen issue](https://technoracle.com/how-to-fix-zoom-screen-sharing-on-ubuntu-22-04-quickly/)

- [x] Try to organize better the GitHub repository so that also Alessio can work on that 
	- [x] Create Obsidian Vault to and add it to the Git Repository

- [ ] Organize better the code to optimize it.
	- [x] Written `make_importance` function in C with `OpenMP` → up to 130 times faster  
	- [x] <span style="color:red;">Add the new features inserted in the plot functions for inserting DIFFI in PyOD</span>
	- [ ]  <span style="color:green;">Reboot and review old code</span> 

- [ ] Reboot and review experiments code -> write on a Python script 
	- [ ] Create a result table with all the time execution details to see how the model scales with the dataset size. Compare Isolation Forest with EIF and EIF+ and other AD models (e.g. a state of the art AD AutoEncoder and Deep Isolation Forest). Metrics to use for the comparison: AUC ROC Score, Average Precision, Precision, Recall, F1 Score, ... and the time values,`real_time`,and `user_time`)
	
- [x] Test the implementation of DIF,Autoencoder ,(INNE?) of PyOD. See [here](https://pyod.readthedocs.io/en/latest/pyod.models.html) 

	
- [ ] Search a good dataset for discussing the results (think about what kind of experiments to do) with ground truth labels where there is some domain knowledge. We want anomalies to be truly isolated points and not just minority classes in a Multi Class Classification problem. → Some possible examples are [[ExIFFI PAPER REVIEW#Benchmark Datasets|here]]. 

- [ ] Ablation studies of EIF+
	- [ ] Different normalizing distributions (i.e. different scalers other than `StandardScaler` → `MinMaxScaler`,)
	- [ ] Different number of Isolation Trees 
	- [ ] Different distribution for selecting the intercept point $p$ from which the partition hyperplane has to pass 
	- [ ] Try to use median instead of mean to select the intercept point (i.e. $N(median(X),\eta \ std(X)$)
	- [ ] Divide in train and test and change the percentage of anomalies in the training set 

- [ ] Ablation studies ExIFFI 
	- [ ] What happens to explainability as the contamination factor increases ? 
	- [ ] Different number of Isolation Trees 
	- [ ] Include or exclude the `depth_based` parameter 
	- [ ] How interpretation changes giving in input only inliers or datasets with also outliers 
	- [ ] Try to use other percentages (other than 10%) to split the dataset in inliers and outliers when we compute the Global Importance 

- [ ] Deep Isolation Forest and compare it to ExIFFI -> say at the end that the problem with Deep Isolation Forest is that is very difficult to apply interpretability to it. 
	- [ ] Focus more on comparing the feature importance obtained with ExIFFI with the one obtained with Random Forest (train the random forest model with the anomaly scores obtained from the Anomaly Detection models).

- [ ] Search for papers on Feature Importance in Decision Trees/Random Forest and see what kind of experiments they did 

- [ ] Interpretable models evaluation metrics from *Evaluation of post-hoc interpretability methods in time-series classification*
	- [ ] Use TIC Index from the paper *Evaluation of post-hoc interpretability methods in time-series classification* 
	- [ ] Use the metrics $AUC\tilde{S}_{top}$ and $F1\tilde{S}$ to evaluate the feature importance

- [ ] Enlarge the Related Work section 
	- [ ] See some possible paper to cite [[ExIFFI PAPER REVIEW#Papers for Related Work|here]]

- [ ] User Study → In the review they proposed to conduct a [[ExIFFI PAPER REVIEW#^e08017|User Study]] to evaluate the effectiveness of the Interpretability method. This is something a bit complicated to do so, as Gian suggested, we should cite a paper (that Gian will provide us) that described the possible challenges in including such a study and then we say that we decided to use the Feature Selection proxy task (and possibly the metrics defined in paper *Evaluation of post-hoc interpretability methods in time-series classification*) for the evaluation of the Interpretability part. 

## For 15 February  

- [x] <span style="color:red;">Ask Gian for inclusion of Francesco optimized code in the paper</span>
	- [x] There is also Davide Sartor optimized code to take inspiration from

- [x] Organize better the code to optimize it.
	- [x] Written `make_importance` function in C with `OpenMP` → up to 130 times faster  
	- [x] <span style="color:red;">Add the new features inserted in the plot functions for inserting DIFFI in PyOD</span>
	- [x]  <span style="color:green;">Reboot and review old code</span> 
- [x] <span style="color:red;">Test the implementation of DIF,Autoencoder ,(INNE?) of PyOD</span>

- [ ] <span style="color:yellow;">Search a good dataset for discussing the results (think about what kind of experiments to do) with ground truth labels where there is some domain knowledge. We want anomalies to be truly isolated points and not just minority classes in a Multi Class Classification problem. → Some possible examples are </span>[[ExIFFI PAPER REVIEW#Benchmark Datasets|here]]  

- [x] <span style="color:yellow">Draw a scheme for how to run the experiments (take inspiration also on the studies/experiments of other papers) → also for the Ablation Studies</span> → See [[EXPERIMENT SCRIPTS]]. 

## For 22 February

- [x] <span style="color:red">Reboot and review experiments code -> write on a Python script</span>
	- [x] <span style="color:red">Create a result table with all the time execution details to see how the model scales with the dataset size. Compare Isolation Forest with EIF and EIF+ and other AD models (e.g. a state of the art AD AutoEncoder and Deep Isolation Forest). Metrics to use for the comparison: AUC ROC Score, Average Precision, Precision, Recall, F1 Score, ... and the time values,`real_time`,and `user_time`)</span>
- [x] Add a `contamination` parameter in `ExtendedIsolationForest` → all the PyOD models have one. Moreover it would also help in the `_predict` function instead of having to pass it as an input parameter. 

- [ ] <span style="color:green;">Adapt experiments to the `numba` code </span> 
	- [ ] <span style="color:green;">Add experiments on different versions of `X_train` and `X_test` (e.g start with no anomalies in `X_train` and anomalies in `X_test` and then add some anomalies in `X_train`)</span>
- [ ] <span style="color:green;">Implement new version of Feature Selection experiment</span>

- [ ] <span style="color:red;">Check in DIF Paper if they apply a Data Normalization → maybe Data Normalization is not a good idea before applying all the non linear transformation they apply on the *deep network part* of the DIF model.</span>
- [ ] <span style="color:red;">Try to work again on `Test_AD_Models` to get result that have sense on DIF and other models</span>
- [ ] <span style="color:red;">Produce a summary of how the new version of the paper should look like so that we can have a clear idea of how to structure the experiments:</span>
	- [ ] <span style="color:red;">create a new note with the paper structure and experiment structure → take inspiration from the old paper structure and from the review.</span>
	


