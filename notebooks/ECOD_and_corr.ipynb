{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `ECOD` Trial Notebook\n",
    "\n",
    "Let's try to get familiar with the `ECOD` model from `pyod` to then implement all the experiments with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_reboot.datasets import Dataset\n",
    "from utils_reboot.utils import * \n",
    "from model_reboot.interpretability_module import *\n",
    "import os\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ECOD` is pretty easy to use since it has only 2 input parameters:\n",
    "- `contamination`: the percentage of outliers in the dataset\n",
    "- `n_jobs` : the number of parallel jobs to run for the model, but here we will leave it as default to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ECOD` Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('wine', path = '../data/real',feature_names_filepath='../data/')\n",
    "dataset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scenario_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.initialize_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.329e+01, 1.970e+00, 2.680e+00, ..., 1.070e+00, 2.840e+00,\n",
       "        1.270e+03],\n",
       "       [1.430e+01, 1.920e+00, 2.720e+00, ..., 1.070e+00, 2.650e+00,\n",
       "        1.280e+03],\n",
       "       [1.368e+01, 1.830e+00, 2.360e+00, ..., 1.230e+00, 2.870e+00,\n",
       "        9.900e+02],\n",
       "       ...,\n",
       "       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
       "        8.350e+02],\n",
       "       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
       "        8.400e+02],\n",
       "       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
       "        5.600e+02]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "I=ECOD()\n",
    "I.fit(dataset.X_train)\n",
    "scores=I.predict(dataset.X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Experiments \n",
    "\n",
    "Experiment to compute the correlation between `LFI` importance scores and Anomaly Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 07:40:21.673262: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_reboot.datasets import Dataset\n",
    "from utils_reboot.utils import * \n",
    "import os\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=Dataset('wine',path='../data/real/')\n",
    "dataset.drop_duplicates()\n",
    "dataset.pre_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "I=ExtendedIsolationForest(1,200)\n",
    "I.fit(dataset.X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Anomaly Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores=I.predict(dataset.X_test)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute `LFI` scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfi_scores=I.local_importances(dataset.X_test)\n",
    "lfi_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the sum of each row in the `LFI` scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the sum of each row in lfi_scores\n",
    "lfi_scores_sum = np.sum(lfi_scores, axis=1)\n",
    "lfi_scores_sum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N.B → It makes sense\n",
    "\n",
    "The intuition we started from seems to make sense → the maximym sum of the `LFI` scores corresponds to the sample with the highest Anomaly Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of max lfi_scores_sum: 72\n",
      "Index of max scores: 72\n"
     ]
    }
   ],
   "source": [
    "# Find the index where lfi_scores_sum is the highest\n",
    "index = np.argmax(lfi_scores_sum)\n",
    "# Find the index where scores is the highest\n",
    "index2 = np.argmax(scores)\n",
    "print(f'Index of max lfi_scores_sum: {index}')\n",
    "print(f'Index of max scores: {index2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of min lfi_scores_sum: 99\n",
      "Index of min scores: 99\n"
     ]
    }
   ],
   "source": [
    "# Find the index where lfi_scores_sum is the lowest\n",
    "index = np.argmin(lfi_scores_sum)\n",
    "# Find the index where scores is the lowest\n",
    "index2 = np.argmin(scores)\n",
    "print(f'Index of min lfi_scores_sum: {index}')\n",
    "print(f'Index of min scores: {index2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Correlation\n",
    "\n",
    "The correlation score is almost 1 so this confirms the intuition we started from → the higher the feature importances the higher the Anomaly Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between scores and lfi_scores_sum: 0.9621052097500566\n"
     ]
    }
   ],
   "source": [
    "# Compute the correlation between scores and lfi_scores_sum\n",
    "correlation = np.corrcoef(scores, lfi_scores_sum)\n",
    "print(f'Correlation between scores and lfi_scores_sum: {correlation[0,1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.96210521],\n",
       "       [0.96210521, 1.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check `corr_exp.pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath=os.path.dirname(os.getcwd())\n",
    "cor_path=os.path.join(basepath,'utils_reboot','corr_exp.pickle')\n",
    "cor_dict=open_element(cor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['EXIFFI+', 'EXIFFI', 'DIFFI', 'IF_RandomForest', 'EIF_RandomForest', 'EIF+_RandomForest', 'RandomForest'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Xaxis'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_dict['EXIFFI+'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9284276502821726,\n",
       " 0.936314335211612,\n",
       " 0.9158891873347039,\n",
       " 0.9364476213950881,\n",
       " 0.9314721781297911,\n",
       " 0.9289341147883154,\n",
       " 0.918314225129243,\n",
       " 0.9168395835342296,\n",
       " 0.9356587095881654,\n",
       " 0.9271933197199017]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_dict['EXIFFI+']['Xaxis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with `DIFFI` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 09:00:54.003925: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_reboot.datasets import Dataset\n",
    "from utils_reboot.utils import * \n",
    "from model_reboot.interpretability_module import *\n",
    "import os\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=Dataset('wine',path='../data/real/')\n",
    "dataset.drop_duplicates()\n",
    "dataset.pre_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>sklearn_IsolationForest()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">sklearn_IsolationForest</label><div class=\"sk-toggleable__content\"><pre>sklearn_IsolationForest()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "sklearn_IsolationForest()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I=sklearn_IsolationForest(n_estimators=200)\n",
    "I.fit(dataset.X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Anomaly Scores with `DIFFI` and check the correlation with the `LFI` scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=I.predict(dataset.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfi=np.zeros((dataset.X_test.shape[0],dataset.X_test.shape[1]))\n",
    "for i in range(dataset.X_test.shape[0]):\n",
    "    lfi[i],_=local_diffi(I,dataset.X_test[i,:])\n",
    "\n",
    "lfi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfi_sum=np.sum(lfi,axis=1)\n",
    "lfi_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between scores and lfi_scores_sum: 0.9406390411642097\n"
     ]
    }
   ],
   "source": [
    "correlation = np.corrcoef(scores, lfi_sum)\n",
    "print(f'Correlation between scores and lfi_scores_sum: {correlation[0,1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `RandomForest` Importance\n",
    "\n",
    "The strategy used throughout the paper to compute the importance with `RandomForest` is to create a `RandomForestRegressor()` and fit it to the task of predicting the Anomaly Scores produced by the `AD` model we want to explain. Then we compute the feature importances with the `feature_importances_` attribute of the `RandomForestRegressor` model.\n",
    "\n",
    "This is however only a `GFI` score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(dataset.X_test, I.predict(dataset.X_test))\n",
    "fi = rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06552875, 0.02920797, 0.07393253, 0.0663468 , 0.19107696,\n",
       "       0.1586969 , 0.02763913, 0.04929794, 0.1489197 , 0.04123027,\n",
       "       0.04694226, 0.0642726 , 0.03690819])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N.B. `RandomForest` `LFI` Score\n",
    "\n",
    "One possible idea to compute the `LFI` scores with the `RandomForest` surrogate model is to fit the model only on a single sample and then compute the feature importances. This way we can get the `LFI` scores for a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Feature Importance: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(dataset.X_test[10,:].reshape(1,-1), [I.predict(dataset.X_test)[10]])\n",
    "fi = rf.feature_importances_\n",
    "print(f'Local Feature Importance: {fi}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.76126556, -0.39331144,  1.22362398, -1.20140666,  0.30838901,\n",
       "         1.5701678 ,  1.73351092, -0.63975504,  0.33709255,  0.42461294,\n",
       "         0.61469876,  0.65569271,  2.92914239]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.X_test[0,:].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4777249886806976]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[I.predict(dataset.X_test)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ECOD` Feature Importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 12:38:52.586488: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_reboot.datasets import Dataset\n",
    "from utils_reboot.utils import * \n",
    "from model_reboot.interpretability_module import *\n",
    "import os\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('breastw', path = '../data/real',feature_names_filepath='../data/')\n",
    "dataset.drop_duplicates()\n",
    "dataset.initialize_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "I=ECOD()\n",
    "I.fit(dataset.X_train)\n",
    "scores=I.predict(dataset.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Global Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gfi=I.ecod_global_importance(dataset.X_test)\n",
    "gfi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37449071, 0.55697087, 0.54058565, ..., 0.28729595, 0.45068159,\n",
       "        0.24038257],\n",
       "       [0.37449071, 0.4592484 , 0.41984699, ..., 0.28729595, 0.42203224,\n",
       "        0.24038257],\n",
       "       [0.5145696 , 0.55697087, 0.54058565, ..., 0.28729595, 0.45068159,\n",
       "        0.24038257],\n",
       "       ...,\n",
       "       [0.37449071, 1.        , 1.        , ..., 0.48035235, 1.        ,\n",
       "        0.31846463],\n",
       "       [0.44556845, 0.70386223, 0.50540924, ..., 1.        , 0.54996113,\n",
       "        0.24038257],\n",
       "       [0.44556845, 0.70386223, 0.67821533, ..., 1.        , 0.48568515,\n",
       "        0.24038257]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N.B. \n",
    "\n",
    "- Attribute `O` → Outlier score for each feature → take onl the first half of the scores \n",
    "- Formula for feature importance\n",
    " -  For each feature f\n",
    "    - Distance from a given sample `x[f]` and the 99% percentile of feature `f` \n",
    "    - The importance score is the inverse of this distance → in this way the closer a sample is to the 99% percentile the higher the importance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(898, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.O.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50490407, 1.13720959, 1.23182556, 0.97122445, 0.85474946,\n",
       "       0.95952841, 0.63055934, 0.79381691, 0.30793023])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.O[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50490407, 1.13720959, 1.23182556, 0.97122445, 0.85474946,\n",
       "       0.95952841, 0.63055934, 0.79381691, 0.30793023])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.O[449]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.275273</td>\n",
       "      <td>1.167302</td>\n",
       "      <td>1.206074</td>\n",
       "      <td>1.134320</td>\n",
       "      <td>1.222775</td>\n",
       "      <td>1.000107</td>\n",
       "      <td>1.295681</td>\n",
       "      <td>1.069922</td>\n",
       "      <td>0.771913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.559160</td>\n",
       "      <td>0.397239</td>\n",
       "      <td>0.428647</td>\n",
       "      <td>0.450031</td>\n",
       "      <td>0.706675</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.635557</td>\n",
       "      <td>0.456634</td>\n",
       "      <td>0.859938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.504904</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.686488</td>\n",
       "      <td>0.686488</td>\n",
       "      <td>0.554063</td>\n",
       "      <td>0.668944</td>\n",
       "      <td>0.630559</td>\n",
       "      <td>0.643191</td>\n",
       "      <td>0.307930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.925239</td>\n",
       "      <td>0.875914</td>\n",
       "      <td>0.914066</td>\n",
       "      <td>0.947968</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.959528</td>\n",
       "      <td>0.759915</td>\n",
       "      <td>0.793817</td>\n",
       "      <td>0.307930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.231826</td>\n",
       "      <td>1.137210</td>\n",
       "      <td>1.231826</td>\n",
       "      <td>0.971224</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.959528</td>\n",
       "      <td>1.130289</td>\n",
       "      <td>0.793817</td>\n",
       "      <td>0.307930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.872916</td>\n",
       "      <td>1.336338</td>\n",
       "      <td>1.319531</td>\n",
       "      <td>1.336338</td>\n",
       "      <td>1.472294</td>\n",
       "      <td>1.239488</td>\n",
       "      <td>1.830357</td>\n",
       "      <td>1.370824</td>\n",
       "      <td>1.327899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.175197</td>\n",
       "      <td>1.932636</td>\n",
       "      <td>2.081671</td>\n",
       "      <td>2.099690</td>\n",
       "      <td>2.848926</td>\n",
       "      <td>1.239488</td>\n",
       "      <td>3.111291</td>\n",
       "      <td>2.012678</td>\n",
       "      <td>3.467966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2           3           4           5  \\\n",
       "count  898.000000  898.000000  898.000000  898.000000  898.000000  898.000000   \n",
       "mean     1.275273    1.167302    1.206074    1.134320    1.222775    1.000107   \n",
       "std      0.559160    0.397239    0.428647    0.450031    0.706675    0.182251   \n",
       "min      0.504904    0.634752    0.686488    0.686488    0.554063    0.668944   \n",
       "25%      0.925239    0.875914    0.914066    0.947968    0.854749    0.959528   \n",
       "50%      1.231826    1.137210    1.231826    0.971224    0.854749    0.959528   \n",
       "75%      1.872916    1.336338    1.319531    1.336338    1.472294    1.239488   \n",
       "max      2.175197    1.932636    2.081671    2.099690    2.848926    1.239488   \n",
       "\n",
       "                6           7           8  \n",
       "count  898.000000  898.000000  898.000000  \n",
       "mean     1.295681    1.069922    0.771913  \n",
       "std      0.635557    0.456634    0.859938  \n",
       "min      0.630559    0.643191    0.307930  \n",
       "25%      0.759915    0.793817    0.307930  \n",
       "50%      1.130289    0.793817    0.307930  \n",
       "75%      1.830357    1.370824    1.327899  \n",
       "max      3.111291    2.012678    3.467966  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_df=pd.DataFrame(I.O)\n",
    "imp_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.67029319, -0.79542603, -0.84984563, ..., -2.48073128,\n",
       "        -1.21886142, -3.16003532],\n",
       "       [-1.67029319, -1.17747086, -1.38182008, ..., -2.48073128,\n",
       "        -1.36948724, -3.16003532],\n",
       "       [-0.94337169, -0.79542603, -0.84984563, ..., -2.48073128,\n",
       "        -1.21886142, -3.16003532],\n",
       "       ...,\n",
       "       [-1.67029319,  0.        ,  0.        , ..., -1.08180517,\n",
       "         0.        , -2.14006616],\n",
       "       [-1.2443241 , -0.42073258, -0.97859462, ...,  0.        ,\n",
       "        -0.81831032, -3.16003532],\n",
       "       [-1.2443241 , -0.42073258, -0.47445798, ...,  0.        ,\n",
       "        -1.05894703, -3.16003532]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.O - np.quantile(I.O, 0.99, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL FORMULA TO IMPLEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37449071, 0.43518309, 0.54058565, 0.41669469, 0.3339816 ,\n",
       "       0.7812744 , 0.33545773, 0.45068159, 0.24038257])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_weight=1/(1+(np.quantile(I.O, 0.99, axis=0) - I.O))\n",
    "imp_weight[70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyod_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
